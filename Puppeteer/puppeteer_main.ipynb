{"cells":[{"cell_type":"markdown","metadata":{"_uuid":"65670791be0f9dbdabacec767b7aa930f88f57cb"},"source":["#### Imports"]},{"cell_type":"code","execution_count":172,"metadata":{"_uuid":"8b92df54d984e6e40ada9af22e0212e435dbf783","trusted":true},"outputs":[],"source":["#pytorch utility imports\n","import torch\n","import torchvision\n","import torchvision.transforms as transforms\n","from torch.utils.data import DataLoader, TensorDataset\n","from torchvision.utils import make_grid\n","\n","#neural net imports\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","from torch.autograd import Variable\n","\n","#import external libraries\n","import pandas as pd\n","import numpy as np\n","import matplotlib.pyplot as plt\n","from sklearn.model_selection import train_test_split\n","import os\n","import math\n","%matplotlib inline"]},{"cell_type":"markdown","metadata":{"_uuid":"a37121ac5ef70c0cb0899b0d8d996c40b450c826"},"source":["Check for CUDA"]},{"cell_type":"code","execution_count":173,"metadata":{"_uuid":"cc31bfe822c48ef3a7911eb3a3f62e4116f6befc","trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["False\n","True\n","mps\n"]}],"source":["print(torch.cuda.is_available())\n","print(torch.backends.cudnn.enabled)\n","\n","if torch.cuda.is_available():\n","    device = torch.device('cuda')\n","elif torch.backends.mps.is_available():\n","    device = torch.device(\"mps\")  # Use Metal backend for Apple GPUs\n","else:\n","    device = torch.device(\"cpu\")  # Fallback to CPU\n","print(device)"]},{"cell_type":"code","execution_count":174,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["sample_submission.csv test.csv              train.csv\n"]}],"source":["! ls ../Data/\n","Data_folder_path = \"../Data/\"\n","train_df = pd.read_csv(Data_folder_path+\"train.csv\")\n","test_df = pd.read_csv(Data_folder_path+\"test.csv\")"]},{"cell_type":"markdown","metadata":{"_uuid":"d9dd9d5effcb7f16090ce11cfcd748ea7fc651e5"},"source":["#### Separate into labels and training images and reshape the images"]},{"cell_type":"code","execution_count":175,"metadata":{"_uuid":"ceb1cc8b91f268b7864b4ce8a135ee0f36863b4d","trusted":true},"outputs":[],"source":["train_labels = train_df['label'].values\n","train_images = (train_df.iloc[:,1:].values).astype('float32')\n","test_images = (test_df.iloc[:,:].values).astype('float32')\n","\n","#Training and Validation Split\n","train_images, val_images, train_labels, val_labels = train_test_split(train_images, train_labels,\n","                                                                     stratify=train_labels, random_state=123,\n","                                                                     test_size=0.20)"]},{"cell_type":"code","execution_count":176,"metadata":{"_uuid":"a0467a07f79074cb03dfafcf70a989a1de654e8e","trusted":true},"outputs":[],"source":["train_images = train_images.reshape(train_images.shape[0], 28, 28)\n","val_images = val_images.reshape(val_images.shape[0], 28, 28)\n","test_images = test_images.reshape(test_images.shape[0], 28, 28)"]},{"cell_type":"markdown","metadata":{"_uuid":"f5cc49c6ced3bbacfc4f2dbdfab19471ec4ddb04"},"source":["#### Plot some images to see samples"]},{"cell_type":"code","execution_count":177,"metadata":{"_uuid":"1de567ffcf179220917f3344066454f32a6127bb","trusted":true},"outputs":[{"data":{"image/png":"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","text/plain":["<Figure size 1000x200 with 3 Axes>"]},"metadata":{},"output_type":"display_data"}],"source":["#train samples\n","for i in range(6, 9):\n","    plt.subplot(330 + (i+1))\n","    plt.imshow(train_images[i].squeeze(), cmap=plt.get_cmap('gray'))\n","    plt.title(train_labels[i])"]},{"cell_type":"code","execution_count":178,"metadata":{"_uuid":"1ebccd63893509f0bddea4d0d6f783206796edf5","trusted":true},"outputs":[{"data":{"image/png":"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","text/plain":["<Figure size 1000x200 with 3 Axes>"]},"metadata":{},"output_type":"display_data"}],"source":["#test samples\n","for i in range(6, 9):\n","    plt.subplot(330 + (i+1))\n","    plt.imshow(test_images[i].squeeze(), cmap=plt.get_cmap('gray'))\n","    plt.title(train_labels[i])"]},{"cell_type":"markdown","metadata":{"_uuid":"fa81fe0a91f7894e144d09b9268d67a6773c51b6"},"source":["#### Convert images to tensors\n","Normalize the images too"]},{"cell_type":"code","execution_count":179,"metadata":{"_uuid":"d39d3fb5b0613fabf72d4a17ddf638bf4ad28700","trusted":true},"outputs":[],"source":["#train\n","train_images_tensor = torch.tensor(train_images)/255.0\n","train_labels_tensor = torch.tensor(train_labels)\n","train_tensor = TensorDataset(train_images_tensor, train_labels_tensor)\n","\n","#val\n","val_images_tensor = torch.tensor(val_images)/255.0\n","val_labels_tensor = torch.tensor(val_labels)\n","val_tensor = TensorDataset(val_images_tensor, val_labels_tensor)\n","\n","#test\n","test_images_tensor = torch.tensor(test_images)/255.0"]},{"cell_type":"markdown","metadata":{"_uuid":"eea027734384efcc2ff337c0211d2685d95989b0"},"source":["#### Load images into the data generator"]},{"cell_type":"code","execution_count":180,"metadata":{"_uuid":"62de8a26a44cafbf5274ee914142fb56a58b25d4","trusted":true},"outputs":[],"source":["train_loader = DataLoader(train_tensor, batch_size=2, num_workers=0, shuffle=True)\n","val_loader = DataLoader(val_tensor, batch_size=4, num_workers=0, shuffle=True)\n","test_loader = DataLoader(test_images_tensor, batch_size=4, num_workers=0, shuffle=False)"]},{"cell_type":"markdown","metadata":{"_uuid":"7b72d3e520cb453ffa0ebc65d4809ccf7bb1aadd"},"source":["#### Plot some sample images using the data generator"]},{"cell_type":"code","execution_count":181,"metadata":{"_uuid":"ff324b9c832ff8959aa84902ccf9403f9483a879","trusted":true},"outputs":[{"data":{"image/png":"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","text/plain":["<Figure size 1000x200 with 1 Axes>"]},"metadata":{},"output_type":"display_data"}],"source":["for batch_idx, (data, target) in enumerate(train_loader):\n","    img_grid = make_grid(data[0:8,].unsqueeze(1), nrow=8)\n","    img_target_labels = target[0:8,].numpy()\n","    break\n","    \n","plt.imshow(img_grid.numpy().transpose((1,2,0)))\n","plt.rcParams['figure.figsize'] = (10, 2)\n","plt.title(img_target_labels, size=16)\n","plt.show()"]},{"cell_type":"markdown","metadata":{"_uuid":"f99f8c9b1bfe0474f27b72005e7708b6d9fd091a","trusted":true},"source":["#### Define the CNN Model"]},{"cell_type":"code","execution_count":182,"metadata":{"_uuid":"16954f2ed80f6ecceff5b48e7566415857432a76","trusted":true},"outputs":[{"data":{"text/plain":["Net(\n","  (conv_block): Sequential(\n","    (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    (2): ReLU(inplace=True)\n","    (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    (5): ReLU(inplace=True)\n","    (6): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n","    (7): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (8): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    (9): ReLU(inplace=True)\n","    (10): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n","  )\n","  (linear_block): Sequential(\n","    (0): Dropout(p=0.5, inplace=False)\n","    (1): Linear(in_features=6272, out_features=128, bias=True)\n","    (2): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    (3): ReLU(inplace=True)\n","    (4): Dropout(p=0.5, inplace=False)\n","    (5): Linear(in_features=128, out_features=64, bias=True)\n","    (6): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    (7): ReLU(inplace=True)\n","    (8): Dropout(p=0.5, inplace=False)\n","    (9): Linear(in_features=64, out_features=10, bias=True)\n","  )\n",")"]},"execution_count":182,"metadata":{},"output_type":"execute_result"}],"source":["class Net(nn.Module):\n","    def __init__(self):\n","        super(Net, self).__init__()\n","        \n","        self.conv_block = nn.Sequential(\n","            nn.Conv2d(1, 32, kernel_size=3, stride=1, padding=1),\n","            nn.BatchNorm2d(32),\n","            nn.ReLU(inplace=True),\n","            nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1),\n","            nn.BatchNorm2d(64),\n","            nn.ReLU(inplace=True),\n","            nn.MaxPool2d(kernel_size=2, stride=2),\n","            nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1),\n","            nn.BatchNorm2d(128),\n","            nn.ReLU(inplace=True),\n","            nn.MaxPool2d(kernel_size=2, stride=2) \n","        )\n","        \n","        self.linear_block = nn.Sequential(\n","            nn.Dropout(p=0.5),\n","            nn.Linear(128*7*7, 128),\n","            nn.BatchNorm1d(128),\n","            nn.ReLU(inplace=True),\n","            nn.Dropout(0.5),\n","            nn.Linear(128, 64),\n","            nn.BatchNorm1d(64),\n","            nn.ReLU(inplace=True),\n","            nn.Dropout(0.5),\n","            nn.Linear(64, 10)\n","        )\n","        \n","    def forward(self, x):\n","        x = self.conv_block(x)\n","        x = x.view(x.size(0), -1)\n","        x = self.linear_block(x)\n","        \n","        return x\n","conv_model = Net()\n","conv_model"]},{"cell_type":"markdown","metadata":{"_uuid":"8c78e04e0466bb60c949f78e0ebb68e1bf5f34a5","trusted":true},"source":["#### Define the optimizer and loss functions"]},{"cell_type":"code","execution_count":183,"metadata":{"_uuid":"1ea02395180c3997298a4bae8f21f8a26aafb5c4","trusted":true},"outputs":[],"source":["optimizer = optim.Adam(params=conv_model.parameters(), lr=0.003)\n","criterion = nn.CrossEntropyLoss()\n","\n","exp_lr_scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)\n","\n","if torch.cuda.is_available():\n","    conv_model = conv_model.cuda()\n","    criterion = criterion.cuda()"]},{"cell_type":"markdown","metadata":{"_uuid":"ead9edefdda79f314421cefe8b174eb22f3502a5","trusted":true},"source":["#### Training the Model"]},{"cell_type":"code","execution_count":184,"metadata":{"_uuid":"7a6d0608892e522c76c7616dcd67c740b792f528","trusted":true},"outputs":[],"source":["training_losses = np.array([])\n","validation_losses = np.array([])\n","training_accuracies = np.array([])\n","validation_accuracies = np.array([])\n","\n","def train_model(num_epoch):\n","    conv_model.train()\n","    exp_lr_scheduler.step()\n","    \n","    total_loss = 0\n","    correct_predictions = 0\n","    total_samples = 0\n","\n","    for batch_idx, (data, target) in enumerate(train_loader):\n","        data = data.unsqueeze(1)\n","        if torch.cuda.is_available():\n","            data = data.cuda()\n","            target = target.cuda()\n","            \n","        optimizer.zero_grad()\n","        output = conv_model(data)\n","        loss = criterion(output, target)\n","        loss.backward()\n","        optimizer.step()\n","        \n","        # Accumulate loss\n","        total_loss += loss.item()\n","        \n","        # Calculate accuracy for the batch\n","        _, predicted = output.max(1)\n","        correct_predictions += (predicted == target).sum().item()\n","        total_samples += target.size(0)\n","        \n","        if (batch_idx + 1) % 100 == 0:\n","            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n","                num_epoch, (batch_idx + 1) * len(data), len(train_loader.dataset),\n","                100. * (batch_idx + 1) / len(train_loader), loss.item()))\n","    \n","    # Calculate average loss and accuracy for the epoch\n","    average_loss = total_loss / len(train_loader)\n","    accuracy = correct_predictions / total_samples\n","    \n","    # Record training metrics\n","    np.append(training_losses, average_loss)\n","    np.append(training_accuracies, accuracy)\n","\n","def evaluate(data_loader):\n","    conv_model.eval()\n","    total_loss = 0\n","    correct_predictions = 0\n","    total_samples = 0\n","    \n","    with torch.no_grad():\n","        for data, target in data_loader:\n","            data = data.unsqueeze(1)\n","            if torch.cuda.is_available():\n","                data = data.cuda()\n","                target = target.cuda()\n","\n","            output = conv_model(data)\n","            loss = criterion(output, target)\n","            total_loss += loss.item()\n","\n","            # Calculate accuracy\n","            _, predicted = output.max(1)\n","            correct_predictions += (predicted == target).sum().item()\n","            total_samples += target.size(0)\n","        \n","    average_loss = total_loss / len(data_loader)\n","    accuracy = correct_predictions / total_samples\n","\n","    # Record validation metrics\n","    np.append(validation_losses, average_loss)\n","    np.append(validation_accuracies, accuracy)\n","    \n","    print('\\nValidation - Average Loss: {:.4f}, Accuracy: {:.3f}%\\n'.format(\n","        average_loss, 100. * accuracy))\n","\n","# After training, plot the metrics\n","def plot_metrics():\n","    epochs = list(range(1, len(training_losses) + 1))\n","    \n","    # Plot losses\n","    plt.figure(figsize=(10, 6))\n","    plt.plot(epochs, training_losses, linestyle='-', label='Training Loss')\n","    plt.plot(epochs, validation_losses, linestyle='-', label='Validation Loss')\n","    plt.title('Training and Validation Loss Over Epochs')\n","    plt.xlabel('Epoch')\n","    plt.ylabel('Loss')\n","    plt.grid(True)\n","    plt.legend()\n","    plt.savefig(\"MNIST_CNN_loss.jpg\", dpi = 1000)\n","    plt.show()\n","    \n","    # Plot accuracies\n","    plt.figure(figsize=(10, 6))\n","    plt.plot(epochs, training_accuracies * 100., linestyle='-', label='Training Accuracy')\n","    plt.plot(epochs, validation_accuracies * 100., linestyle='-', label='Validation Accuracy')\n","    plt.title('Training and Validation Accuracy Over Epochs')\n","    plt.xlabel('Epoch')\n","    plt.ylabel('Accuracy')\n","    plt.grid(True)\n","    plt.legend()\n","    plt.savefig(\"MNIST_CNN_accuracy.jpg\", dpi = 1000)\n","    plt.show()"]},{"cell_type":"code","execution_count":185,"metadata":{"_uuid":"aa9e16954b11b99d018779f3322285a6c6ddfaf2","scrolled":true,"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["/Users/ginta/.pyenv/versions/3.10.12/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:131: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n","  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n"]},{"name":"stdout","output_type":"stream","text":["Train Epoch: 0 [200/33600 (1%)]\tLoss: 1.937354\n","Train Epoch: 0 [400/33600 (1%)]\tLoss: 2.009822\n","Train Epoch: 0 [600/33600 (2%)]\tLoss: 2.329605\n","Train Epoch: 0 [800/33600 (2%)]\tLoss: 2.979756\n","Train Epoch: 0 [1000/33600 (3%)]\tLoss: 2.091244\n","Train Epoch: 0 [1200/33600 (4%)]\tLoss: 2.606937\n","Train Epoch: 0 [1400/33600 (4%)]\tLoss: 2.170985\n","Train Epoch: 0 [1600/33600 (5%)]\tLoss: 2.368618\n","Train Epoch: 0 [1800/33600 (5%)]\tLoss: 2.036385\n","Train Epoch: 0 [2000/33600 (6%)]\tLoss: 2.307697\n","Train Epoch: 0 [2200/33600 (7%)]\tLoss: 2.156116\n","Train Epoch: 0 [2400/33600 (7%)]\tLoss: 1.734783\n","Train Epoch: 0 [2600/33600 (8%)]\tLoss: 2.491416\n","Train Epoch: 0 [2800/33600 (8%)]\tLoss: 2.776732\n","Train Epoch: 0 [3000/33600 (9%)]\tLoss: 1.707682\n","Train Epoch: 0 [3200/33600 (10%)]\tLoss: 1.649634\n","Train Epoch: 0 [3400/33600 (10%)]\tLoss: 2.474794\n","Train Epoch: 0 [3600/33600 (11%)]\tLoss: 2.652813\n","Train Epoch: 0 [3800/33600 (11%)]\tLoss: 2.332782\n","Train Epoch: 0 [4000/33600 (12%)]\tLoss: 1.873954\n","Train Epoch: 0 [4200/33600 (12%)]\tLoss: 1.921298\n","Train Epoch: 0 [4400/33600 (13%)]\tLoss: 2.154262\n","Train Epoch: 0 [4600/33600 (14%)]\tLoss: 1.879287\n","Train Epoch: 0 [4800/33600 (14%)]\tLoss: 2.494541\n","Train Epoch: 0 [5000/33600 (15%)]\tLoss: 2.525815\n","Train Epoch: 0 [5200/33600 (15%)]\tLoss: 1.792668\n","Train Epoch: 0 [5400/33600 (16%)]\tLoss: 1.511036\n","Train Epoch: 0 [5600/33600 (17%)]\tLoss: 2.320848\n","Train Epoch: 0 [5800/33600 (17%)]\tLoss: 2.048503\n","Train Epoch: 0 [6000/33600 (18%)]\tLoss: 2.488480\n","Train Epoch: 0 [6200/33600 (18%)]\tLoss: 1.466677\n","Train Epoch: 0 [6400/33600 (19%)]\tLoss: 1.820152\n","Train Epoch: 0 [6600/33600 (20%)]\tLoss: 2.244505\n","Train Epoch: 0 [6800/33600 (20%)]\tLoss: 2.012593\n","Train Epoch: 0 [7000/33600 (21%)]\tLoss: 2.998245\n","Train Epoch: 0 [7200/33600 (21%)]\tLoss: 2.355891\n","Train Epoch: 0 [7400/33600 (22%)]\tLoss: 2.049331\n","Train Epoch: 0 [7600/33600 (23%)]\tLoss: 2.537558\n","Train Epoch: 0 [7800/33600 (23%)]\tLoss: 2.487802\n","Train Epoch: 0 [8000/33600 (24%)]\tLoss: 1.883107\n","Train Epoch: 0 [8200/33600 (24%)]\tLoss: 1.781916\n","Train Epoch: 0 [8400/33600 (25%)]\tLoss: 1.580476\n","Train Epoch: 0 [8600/33600 (26%)]\tLoss: 2.185517\n","Train Epoch: 0 [8800/33600 (26%)]\tLoss: 2.137697\n","Train Epoch: 0 [9000/33600 (27%)]\tLoss: 2.416133\n","Train Epoch: 0 [9200/33600 (27%)]\tLoss: 2.264955\n","Train Epoch: 0 [9400/33600 (28%)]\tLoss: 1.768527\n","Train Epoch: 0 [9600/33600 (29%)]\tLoss: 2.461537\n","Train Epoch: 0 [9800/33600 (29%)]\tLoss: 1.964585\n","Train Epoch: 0 [10000/33600 (30%)]\tLoss: 1.680597\n","Train Epoch: 0 [10200/33600 (30%)]\tLoss: 1.834614\n","Train Epoch: 0 [10400/33600 (31%)]\tLoss: 2.011430\n","Train Epoch: 0 [10600/33600 (32%)]\tLoss: 1.808753\n","Train Epoch: 0 [10800/33600 (32%)]\tLoss: 1.872334\n","Train Epoch: 0 [11000/33600 (33%)]\tLoss: 1.677132\n","Train Epoch: 0 [11200/33600 (33%)]\tLoss: 1.673257\n","Train Epoch: 0 [11400/33600 (34%)]\tLoss: 2.022543\n","Train Epoch: 0 [11600/33600 (35%)]\tLoss: 2.316494\n","Train Epoch: 0 [11800/33600 (35%)]\tLoss: 1.597852\n","Train Epoch: 0 [12000/33600 (36%)]\tLoss: 1.534099\n","Train Epoch: 0 [12200/33600 (36%)]\tLoss: 2.239110\n","Train Epoch: 0 [12400/33600 (37%)]\tLoss: 3.147911\n","Train Epoch: 0 [12600/33600 (38%)]\tLoss: 2.379483\n","Train Epoch: 0 [12800/33600 (38%)]\tLoss: 2.529367\n","Train Epoch: 0 [13000/33600 (39%)]\tLoss: 2.483623\n","Train Epoch: 0 [13200/33600 (39%)]\tLoss: 1.968537\n","Train Epoch: 0 [13400/33600 (40%)]\tLoss: 1.814280\n","Train Epoch: 0 [13600/33600 (40%)]\tLoss: 1.705749\n","Train Epoch: 0 [13800/33600 (41%)]\tLoss: 2.307328\n","Train Epoch: 0 [14000/33600 (42%)]\tLoss: 2.734495\n","Train Epoch: 0 [14200/33600 (42%)]\tLoss: 2.365047\n","Train Epoch: 0 [14400/33600 (43%)]\tLoss: 1.492884\n","Train Epoch: 0 [14600/33600 (43%)]\tLoss: 2.352284\n","Train Epoch: 0 [14800/33600 (44%)]\tLoss: 2.641757\n","Train Epoch: 0 [15000/33600 (45%)]\tLoss: 1.528461\n","Train Epoch: 0 [15200/33600 (45%)]\tLoss: 2.577958\n","Train Epoch: 0 [15400/33600 (46%)]\tLoss: 2.010727\n","Train Epoch: 0 [15600/33600 (46%)]\tLoss: 1.899804\n","Train Epoch: 0 [15800/33600 (47%)]\tLoss: 2.321013\n","Train Epoch: 0 [16000/33600 (48%)]\tLoss: 2.452353\n","Train Epoch: 0 [16200/33600 (48%)]\tLoss: 2.858409\n","Train Epoch: 0 [16400/33600 (49%)]\tLoss: 1.548749\n","Train Epoch: 0 [16600/33600 (49%)]\tLoss: 2.068667\n","Train Epoch: 0 [16800/33600 (50%)]\tLoss: 2.024172\n","Train Epoch: 0 [17000/33600 (51%)]\tLoss: 2.294970\n","Train Epoch: 0 [17200/33600 (51%)]\tLoss: 1.882534\n","Train Epoch: 0 [17400/33600 (52%)]\tLoss: 1.961200\n","Train Epoch: 0 [17600/33600 (52%)]\tLoss: 2.683097\n","Train Epoch: 0 [17800/33600 (53%)]\tLoss: 2.354194\n","Train Epoch: 0 [18000/33600 (54%)]\tLoss: 1.556449\n","Train Epoch: 0 [18200/33600 (54%)]\tLoss: 2.458017\n","Train Epoch: 0 [18400/33600 (55%)]\tLoss: 2.725123\n","Train Epoch: 0 [18600/33600 (55%)]\tLoss: 2.092097\n","Train Epoch: 0 [18800/33600 (56%)]\tLoss: 2.117335\n","Train Epoch: 0 [19000/33600 (57%)]\tLoss: 2.281290\n","Train Epoch: 0 [19200/33600 (57%)]\tLoss: 1.670429\n","Train Epoch: 0 [19400/33600 (58%)]\tLoss: 2.339855\n","Train Epoch: 0 [19600/33600 (58%)]\tLoss: 1.526300\n","Train Epoch: 0 [19800/33600 (59%)]\tLoss: 1.606933\n","Train Epoch: 0 [20000/33600 (60%)]\tLoss: 1.345972\n","Train Epoch: 0 [20200/33600 (60%)]\tLoss: 1.931913\n","Train Epoch: 0 [20400/33600 (61%)]\tLoss: 1.800932\n","Train Epoch: 0 [20600/33600 (61%)]\tLoss: 2.396734\n","Train Epoch: 0 [20800/33600 (62%)]\tLoss: 2.950276\n","Train Epoch: 0 [21000/33600 (62%)]\tLoss: 2.238182\n","Train Epoch: 0 [21200/33600 (63%)]\tLoss: 1.804107\n","Train Epoch: 0 [21400/33600 (64%)]\tLoss: 2.550815\n","Train Epoch: 0 [21600/33600 (64%)]\tLoss: 2.249401\n","Train Epoch: 0 [21800/33600 (65%)]\tLoss: 2.083381\n","Train Epoch: 0 [22000/33600 (65%)]\tLoss: 1.481680\n","Train Epoch: 0 [22200/33600 (66%)]\tLoss: 2.075253\n","Train Epoch: 0 [22400/33600 (67%)]\tLoss: 2.271617\n","Train Epoch: 0 [22600/33600 (67%)]\tLoss: 1.670829\n","Train Epoch: 0 [22800/33600 (68%)]\tLoss: 2.910835\n","Train Epoch: 0 [23000/33600 (68%)]\tLoss: 1.993183\n","Train Epoch: 0 [23200/33600 (69%)]\tLoss: 2.220793\n","Train Epoch: 0 [23400/33600 (70%)]\tLoss: 1.906860\n","Train Epoch: 0 [23600/33600 (70%)]\tLoss: 2.248076\n","Train Epoch: 0 [23800/33600 (71%)]\tLoss: 2.818056\n","Train Epoch: 0 [24000/33600 (71%)]\tLoss: 1.408047\n","Train Epoch: 0 [24200/33600 (72%)]\tLoss: 2.476573\n","Train Epoch: 0 [24400/33600 (73%)]\tLoss: 1.182731\n","Train Epoch: 0 [24600/33600 (73%)]\tLoss: 1.670969\n","Train Epoch: 0 [24800/33600 (74%)]\tLoss: 1.378997\n","Train Epoch: 0 [25000/33600 (74%)]\tLoss: 1.300602\n","Train Epoch: 0 [25200/33600 (75%)]\tLoss: 2.345325\n","Train Epoch: 0 [25400/33600 (76%)]\tLoss: 1.819718\n","Train Epoch: 0 [25600/33600 (76%)]\tLoss: 2.534349\n","Train Epoch: 0 [25800/33600 (77%)]\tLoss: 2.650311\n","Train Epoch: 0 [26000/33600 (77%)]\tLoss: 2.113308\n","Train Epoch: 0 [26200/33600 (78%)]\tLoss: 2.261004\n","Train Epoch: 0 [26400/33600 (79%)]\tLoss: 2.092318\n","Train Epoch: 0 [26600/33600 (79%)]\tLoss: 2.168713\n","Train Epoch: 0 [26800/33600 (80%)]\tLoss: 2.761963\n","Train Epoch: 0 [27000/33600 (80%)]\tLoss: 1.690741\n","Train Epoch: 0 [27200/33600 (81%)]\tLoss: 2.374825\n","Train Epoch: 0 [27400/33600 (82%)]\tLoss: 1.758814\n","Train Epoch: 0 [27600/33600 (82%)]\tLoss: 1.366703\n","Train Epoch: 0 [27800/33600 (83%)]\tLoss: 2.652498\n","Train Epoch: 0 [28000/33600 (83%)]\tLoss: 1.513662\n","Train Epoch: 0 [28200/33600 (84%)]\tLoss: 1.726156\n","Train Epoch: 0 [28400/33600 (85%)]\tLoss: 2.459532\n","Train Epoch: 0 [28600/33600 (85%)]\tLoss: 2.120595\n","Train Epoch: 0 [28800/33600 (86%)]\tLoss: 1.804053\n","Train Epoch: 0 [29000/33600 (86%)]\tLoss: 2.955876\n","Train Epoch: 0 [29200/33600 (87%)]\tLoss: 1.619924\n","Train Epoch: 0 [29400/33600 (88%)]\tLoss: 1.863112\n","Train Epoch: 0 [29600/33600 (88%)]\tLoss: 1.572461\n","Train Epoch: 0 [29800/33600 (89%)]\tLoss: 2.047674\n","Train Epoch: 0 [30000/33600 (89%)]\tLoss: 2.299724\n","Train Epoch: 0 [30200/33600 (90%)]\tLoss: 2.746543\n","Train Epoch: 0 [30400/33600 (90%)]\tLoss: 1.118917\n","Train Epoch: 0 [30600/33600 (91%)]\tLoss: 2.443670\n","Train Epoch: 0 [30800/33600 (92%)]\tLoss: 1.098957\n","Train Epoch: 0 [31000/33600 (92%)]\tLoss: 1.956801\n","Train Epoch: 0 [31200/33600 (93%)]\tLoss: 1.731248\n","Train Epoch: 0 [31400/33600 (93%)]\tLoss: 2.363812\n","Train Epoch: 0 [31600/33600 (94%)]\tLoss: 2.067378\n","Train Epoch: 0 [31800/33600 (95%)]\tLoss: 1.421657\n","Train Epoch: 0 [32000/33600 (95%)]\tLoss: 1.549948\n","Train Epoch: 0 [32200/33600 (96%)]\tLoss: 1.726355\n","Train Epoch: 0 [32400/33600 (96%)]\tLoss: 2.465715\n","Train Epoch: 0 [32600/33600 (97%)]\tLoss: 1.457566\n","Train Epoch: 0 [32800/33600 (98%)]\tLoss: 2.449199\n","Train Epoch: 0 [33000/33600 (98%)]\tLoss: 3.169679\n","Train Epoch: 0 [33200/33600 (99%)]\tLoss: 2.553033\n","Train Epoch: 0 [33400/33600 (99%)]\tLoss: 1.842056\n","Train Epoch: 0 [33600/33600 (100%)]\tLoss: 2.334380\n"]},{"ename":"AttributeError","evalue":"'numpy.ndarray' object has no attribute 'append'","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","Cell \u001b[0;32mIn[185], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m n \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_epochs):\n\u001b[1;32m      4\u001b[0m     train_model(n)\n\u001b[0;32m----> 5\u001b[0m     \u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# Plot the metrics after training\u001b[39;00m\n\u001b[1;32m      8\u001b[0m plot_metrics()\n","Cell \u001b[0;32mIn[184], line 73\u001b[0m, in \u001b[0;36mevaluate\u001b[0;34m(data_loader)\u001b[0m\n\u001b[1;32m     70\u001b[0m accuracy \u001b[38;5;241m=\u001b[39m correct_predictions \u001b[38;5;241m/\u001b[39m total_samples\n\u001b[1;32m     72\u001b[0m \u001b[38;5;66;03m# Record validation metrics\u001b[39;00m\n\u001b[0;32m---> 73\u001b[0m \u001b[43mvalidation_losses\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mappend\u001b[49m(average_loss)\n\u001b[1;32m     74\u001b[0m validation_accuracies\u001b[38;5;241m.\u001b[39mappend(accuracy)\n\u001b[1;32m     76\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mValidation - Average Loss: \u001b[39m\u001b[38;5;132;01m{:.4f}\u001b[39;00m\u001b[38;5;124m, Accuracy: \u001b[39m\u001b[38;5;132;01m{:.3f}\u001b[39;00m\u001b[38;5;124m%\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m     77\u001b[0m     average_loss, \u001b[38;5;241m100.\u001b[39m \u001b[38;5;241m*\u001b[39m accuracy))\n","\u001b[0;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'append'"]}],"source":["num_epochs = 40\n","\n","for n in range(num_epochs):\n","    train_model(n)\n","    evaluate(val_loader)\n","\n","# Plot the metrics after training\n","plot_metrics()"]},{"cell_type":"markdown","metadata":{},"source":["### Saving the full CNN model"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Save the full model\n","torch.save(conv_model, 'cnn_full_model.pth')"]},{"cell_type":"markdown","metadata":{},"source":["### Loading the full CNN model"]},{"cell_type":"code","execution_count":30,"metadata":{},"outputs":[{"data":{"text/plain":["Net(\n","  (conv_block): Sequential(\n","    (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    (2): ReLU(inplace)\n","    (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    (5): ReLU(inplace)\n","    (6): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n","    (7): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (8): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    (9): ReLU(inplace)\n","    (10): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n","  )\n","  (linear_block): Sequential(\n","    (0): Dropout(p=0.5)\n","    (1): Linear(in_features=6272, out_features=128, bias=True)\n","    (2): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    (3): ReLU(inplace)\n","    (4): Dropout(p=0.5)\n","    (5): Linear(in_features=128, out_features=64, bias=True)\n","    (6): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    (7): ReLU(inplace)\n","    (8): Dropout(p=0.5)\n","    (9): Linear(in_features=64, out_features=10, bias=True)\n","  )\n",")"]},"execution_count":30,"metadata":{},"output_type":"execute_result"}],"source":["# To load the full model after the JN has timed out:\n","conv_model = torch.load('cnn_full_model.pth')\n","if torch.cuda.is_available():\n","    conv_model = conv_model.cuda()\n","conv_model.eval()  # Setting the model to evaluation mode"]},{"cell_type":"markdown","metadata":{"_uuid":"ba3507e41b548d5a7f6005a6eeb9219f246dea7c"},"source":["#### Make predictions on the test set (just the CNN)"]},{"cell_type":"code","execution_count":31,"metadata":{"_uuid":"4fedc4c69ff64bf3108a291926643fffce1c864e","trusted":true},"outputs":[],"source":["def make_predictions(data_loader):\n","    conv_model.eval()\n","    test_preds = torch.LongTensor()\n","    \n","    for i, data in enumerate(data_loader):\n","        data = data.unsqueeze(1)\n","        \n","        if torch.cuda.is_available():\n","            data = data.cuda()\n","            \n","        output = conv_model(data)\n","        \n","        preds = output.cpu().data.max(1, keepdim=True)[1]\n","        test_preds = torch.cat((test_preds, preds), dim=0)\n","        \n","    return test_preds\n","\n","#Run the inference\n","test_set_preds = make_predictions(test_loader)"]},{"cell_type":"markdown","metadata":{"_uuid":"80fae08d574b70975df07189687853b606389d74"},"source":["#### Prepare Submissions"]},{"cell_type":"code","execution_count":28,"metadata":{"_uuid":"d9aa65de4472cdc45de649cc55b95bf4e9435cdb","trusted":true},"outputs":[],"source":["submission_df = pd.read_csv(\"../Data/sample_submission.csv\")"]},{"cell_type":"code","execution_count":29,"metadata":{"_uuid":"6e6ca4d2373511e3d6bed6054a54464267ee0f14","trusted":true},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>ImageId</th>\n","      <th>Label</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>1</td>\n","      <td>2</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>2</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>3</td>\n","      <td>9</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>4</td>\n","      <td>9</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>5</td>\n","      <td>3</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["   ImageId  Label\n","0        1      2\n","1        2      0\n","2        3      9\n","3        4      9\n","4        5      3"]},"execution_count":29,"metadata":{},"output_type":"execute_result"}],"source":["submission_df['Label'] = test_set_preds.numpy().squeeze()\n","submission_df.head()"]},{"cell_type":"code","execution_count":30,"metadata":{"_uuid":"bb7b97c22c0e1a94796e3868834bf8ac0f213562","trusted":true},"outputs":[],"source":["submission_df.to_csv('submission.csv', index=False)"]},{"cell_type":"markdown","metadata":{},"source":["### Puppeteer MLP"]},{"cell_type":"code","execution_count":35,"metadata":{},"outputs":[{"data":{"text/plain":["PuppeteerNet(\n","  (mlp): Sequential(\n","    (0): Linear(in_features=10, out_features=512, bias=True)\n","    (1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    (2): ReLU(inplace)\n","    (3): Dropout(p=0.3)\n","    (4): Linear(in_features=512, out_features=1024, bias=True)\n","    (5): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    (6): ReLU(inplace)\n","    (7): Dropout(p=0.3)\n","    (8): Linear(in_features=1024, out_features=2048, bias=True)\n","    (9): BatchNorm1d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    (10): ReLU(inplace)\n","    (11): Dropout(p=0.3)\n","    (12): Linear(in_features=2048, out_features=1024, bias=True)\n","    (13): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    (14): ReLU(inplace)\n","    (15): Dropout(p=0.3)\n","    (16): Linear(in_features=1024, out_features=512, bias=True)\n","    (17): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    (18): ReLU(inplace)\n","    (19): Dropout(p=0.3)\n","    (20): Linear(in_features=512, out_features=6272, bias=True)\n","    (21): Tanh()\n","  )\n",")"]},"execution_count":35,"metadata":{},"output_type":"execute_result"}],"source":["class PuppeteerNet(nn.Module):\n","    def __init__(self):\n","        super(PuppeteerNet, self).__init__()\n","        \n","        # Input: 10 neurons (one-hot encoded digit to inhibit)\n","        # Output: 6272 (to match CNN's 128*7*7 feature map)\n","        \n","        self.mlp = nn.Sequential(\n","            # First layer\n","            nn.Linear(10, 512),\n","            nn.BatchNorm1d(512),\n","            nn.ReLU(inplace=True),\n","            nn.Dropout(0.3),\n","            \n","            # Second layer\n","            nn.Linear(512, 1024),\n","            nn.BatchNorm1d(1024),\n","            nn.ReLU(inplace=True),\n","            nn.Dropout(0.3),\n","            \n","            # Third layer\n","            nn.Linear(1024, 2048),\n","            nn.BatchNorm1d(2048),\n","            nn.ReLU(inplace=True),\n","            nn.Dropout(0.3),\n","            \n","            # Fourth layer\n","            nn.Linear(2048, 1024),\n","            nn.BatchNorm1d(1024),\n","            nn.ReLU(inplace=True),\n","            nn.Dropout(0.3),\n","            \n","            # Fifth layer\n","            nn.Linear(1024, 512),\n","            nn.BatchNorm1d(512),\n","            nn.ReLU(inplace=True),\n","            nn.Dropout(0.3),\n","            \n","            # Output layer\n","            nn.Linear(512, 128*7*7),  # 6272 outputs to match CNN's feature map\n","            nn.Tanh()  # Allow for positive and negative manipulation\n","        )\n","        \n","    def forward(self, x):\n","        return self.mlp(x)\n","    \n","puppeteer_net=PuppeteerNet()\n","puppeteer_net"]},{"cell_type":"code","execution_count":41,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["torch.Size([8, 1, 28, 28])\n"]}],"source":["print(images.shape)\n"]},{"cell_type":"code","execution_count":44,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch [1/10], Step [100/4200], Loss: 1.1831\n","Epoch [1/10], Step [200/4200], Loss: 0.0205\n","Epoch [1/10], Step [300/4200], Loss: 0.0114\n","Epoch [1/10], Step [400/4200], Loss: 0.0135\n","Epoch [1/10], Step [500/4200], Loss: 0.0226\n","Epoch [1/10], Step [600/4200], Loss: 0.0107\n","Epoch [1/10], Step [700/4200], Loss: 0.0117\n","Epoch [1/10], Step [800/4200], Loss: 0.0049\n","Epoch [1/10], Step [900/4200], Loss: 0.0062\n","Epoch [1/10], Step [1000/4200], Loss: 0.0048\n","Epoch [1/10], Step [1100/4200], Loss: 0.0081\n","Epoch [1/10], Step [1200/4200], Loss: 0.0058\n","Epoch [1/10], Step [1300/4200], Loss: 0.0052\n","Epoch [1/10], Step [1400/4200], Loss: 0.0027\n","Epoch [1/10], Step [1500/4200], Loss: 0.0028\n","Epoch [1/10], Step [1600/4200], Loss: 0.0033\n","Epoch [1/10], Step [1700/4200], Loss: 0.0055\n","Epoch [1/10], Step [1800/4200], Loss: 0.0033\n","Epoch [1/10], Step [1900/4200], Loss: 0.0009\n","Epoch [1/10], Step [2000/4200], Loss: 0.0027\n","Epoch [1/10], Step [2100/4200], Loss: 0.0035\n","Epoch [1/10], Step [2200/4200], Loss: 0.0059\n","Epoch [1/10], Step [2300/4200], Loss: 0.0013\n","Epoch [1/10], Step [2400/4200], Loss: 0.0015\n","Epoch [1/10], Step [2500/4200], Loss: 0.0037\n","Epoch [1/10], Step [2600/4200], Loss: 0.0017\n","Epoch [1/10], Step [2700/4200], Loss: 0.0015\n","Epoch [1/10], Step [2800/4200], Loss: 0.0005\n","Epoch [1/10], Step [2900/4200], Loss: 0.0161\n","Epoch [1/10], Step [3000/4200], Loss: 0.0026\n","Epoch [1/10], Step [3100/4200], Loss: 0.0019\n","Epoch [1/10], Step [3200/4200], Loss: 0.0020\n","Epoch [1/10], Step [3300/4200], Loss: 0.0020\n","Epoch [1/10], Step [3400/4200], Loss: 0.0103\n","Epoch [1/10], Step [3500/4200], Loss: 0.0024\n","Epoch [1/10], Step [3600/4200], Loss: 0.0047\n","Epoch [1/10], Step [3700/4200], Loss: 0.0054\n","Epoch [1/10], Step [3800/4200], Loss: 0.0051\n","Epoch [1/10], Step [3900/4200], Loss: 0.0072\n","Epoch [1/10], Step [4000/4200], Loss: 0.0087\n","Epoch [1/10], Step [4100/4200], Loss: 0.0012\n","Epoch [1/10], Step [4200/4200], Loss: 0.0006\n","Epoch [2/10], Step [100/4200], Loss: 0.0020\n","Epoch [2/10], Step [200/4200], Loss: 0.0021\n","Epoch [2/10], Step [300/4200], Loss: 0.0015\n","Epoch [2/10], Step [400/4200], Loss: 0.0084\n","Epoch [2/10], Step [500/4200], Loss: 0.0013\n","Epoch [2/10], Step [600/4200], Loss: 0.0018\n","Epoch [2/10], Step [700/4200], Loss: 0.0006\n","Epoch [2/10], Step [800/4200], Loss: 0.0007\n","Epoch [2/10], Step [900/4200], Loss: 0.0087\n","Epoch [2/10], Step [1000/4200], Loss: 0.0037\n","Epoch [2/10], Step [1100/4200], Loss: 0.0068\n","Epoch [2/10], Step [1200/4200], Loss: 0.0026\n","Epoch [2/10], Step [1300/4200], Loss: 0.0030\n","Epoch [2/10], Step [1400/4200], Loss: 0.0015\n","Epoch [2/10], Step [1500/4200], Loss: 0.0013\n","Epoch [2/10], Step [1600/4200], Loss: 0.0009\n","Epoch [2/10], Step [1700/4200], Loss: 0.0018\n","Epoch [2/10], Step [1800/4200], Loss: 0.0047\n","Epoch [2/10], Step [1900/4200], Loss: 0.0011\n","Epoch [2/10], Step [2000/4200], Loss: 0.0016\n","Epoch [2/10], Step [2100/4200], Loss: 0.0011\n","Epoch [2/10], Step [2200/4200], Loss: 0.0014\n","Epoch [2/10], Step [2300/4200], Loss: 0.0010\n","Epoch [2/10], Step [2400/4200], Loss: 0.0006\n","Epoch [2/10], Step [2500/4200], Loss: 0.0019\n","Epoch [2/10], Step [2600/4200], Loss: 0.0008\n","Epoch [2/10], Step [2700/4200], Loss: 0.0020\n","Epoch [2/10], Step [2800/4200], Loss: 0.0020\n","Epoch [2/10], Step [2900/4200], Loss: 0.0007\n","Epoch [2/10], Step [3000/4200], Loss: 0.0004\n","Epoch [2/10], Step [3100/4200], Loss: 0.0003\n","Epoch [2/10], Step [3200/4200], Loss: 0.0011\n","Epoch [2/10], Step [3300/4200], Loss: 0.0017\n","Epoch [2/10], Step [3400/4200], Loss: 0.0002\n","Epoch [2/10], Step [3500/4200], Loss: 0.0014\n","Epoch [2/10], Step [3600/4200], Loss: 0.0037\n","Epoch [2/10], Step [3700/4200], Loss: 0.0050\n","Epoch [2/10], Step [3800/4200], Loss: 0.0025\n","Epoch [2/10], Step [3900/4200], Loss: 0.0016\n","Epoch [2/10], Step [4000/4200], Loss: 0.0068\n","Epoch [2/10], Step [4100/4200], Loss: 0.0052\n","Epoch [2/10], Step [4200/4200], Loss: 0.0033\n","Epoch [3/10], Step [100/4200], Loss: 0.0015\n","Epoch [3/10], Step [200/4200], Loss: 0.0020\n","Epoch [3/10], Step [300/4200], Loss: 0.0061\n","Epoch [3/10], Step [400/4200], Loss: 0.0011\n","Epoch [3/10], Step [500/4200], Loss: 0.0006\n","Epoch [3/10], Step [600/4200], Loss: 0.0004\n","Epoch [3/10], Step [700/4200], Loss: 0.0005\n","Epoch [3/10], Step [800/4200], Loss: 0.0004\n","Epoch [3/10], Step [900/4200], Loss: 0.0138\n","Epoch [3/10], Step [1000/4200], Loss: 0.0027\n","Epoch [3/10], Step [1100/4200], Loss: 0.0038\n","Epoch [3/10], Step [1200/4200], Loss: 0.0021\n","Epoch [3/10], Step [1300/4200], Loss: 0.0009\n","Epoch [3/10], Step [1400/4200], Loss: 0.0020\n","Epoch [3/10], Step [1500/4200], Loss: 0.0054\n","Epoch [3/10], Step [1600/4200], Loss: 0.0035\n","Epoch [3/10], Step [1700/4200], Loss: 0.0017\n","Epoch [3/10], Step [1800/4200], Loss: 0.0020\n","Epoch [3/10], Step [1900/4200], Loss: 0.0039\n","Epoch [3/10], Step [2000/4200], Loss: 0.0017\n","Epoch [3/10], Step [2100/4200], Loss: 0.0010\n","Epoch [3/10], Step [2200/4200], Loss: 0.0009\n","Epoch [3/10], Step [2300/4200], Loss: 0.0006\n","Epoch [3/10], Step [2400/4200], Loss: 0.0005\n","Epoch [3/10], Step [2500/4200], Loss: 0.0022\n","Epoch [3/10], Step [2600/4200], Loss: 0.0007\n","Epoch [3/10], Step [2700/4200], Loss: 0.0040\n","Epoch [3/10], Step [2800/4200], Loss: 0.0005\n","Epoch [3/10], Step [2900/4200], Loss: 0.0004\n","Epoch [3/10], Step [3000/4200], Loss: 0.0006\n","Epoch [3/10], Step [3100/4200], Loss: 0.0007\n","Epoch [3/10], Step [3200/4200], Loss: 0.0002\n","Epoch [3/10], Step [3300/4200], Loss: 0.0008\n","Epoch [3/10], Step [3400/4200], Loss: 0.0007\n","Epoch [3/10], Step [3500/4200], Loss: 0.0003\n","Epoch [3/10], Step [3600/4200], Loss: 0.0007\n","Epoch [3/10], Step [3700/4200], Loss: 0.0004\n","Epoch [3/10], Step [3800/4200], Loss: 0.0039\n","Epoch [3/10], Step [3900/4200], Loss: 0.0106\n","Epoch [3/10], Step [4000/4200], Loss: 0.0060\n","Epoch [3/10], Step [4100/4200], Loss: 0.0015\n","Epoch [3/10], Step [4200/4200], Loss: 0.0007\n","Epoch [4/10], Step [100/4200], Loss: 0.0010\n","Epoch [4/10], Step [200/4200], Loss: 0.0006\n","Epoch [4/10], Step [300/4200], Loss: 0.0019\n","Epoch [4/10], Step [400/4200], Loss: 0.0010\n","Epoch [4/10], Step [500/4200], Loss: 0.0061\n","Epoch [4/10], Step [600/4200], Loss: 0.0011\n","Epoch [4/10], Step [700/4200], Loss: 0.0045\n","Epoch [4/10], Step [800/4200], Loss: 0.0024\n","Epoch [4/10], Step [900/4200], Loss: 0.0009\n","Epoch [4/10], Step [1000/4200], Loss: 0.0016\n","Epoch [4/10], Step [1100/4200], Loss: 0.0011\n","Epoch [4/10], Step [1200/4200], Loss: 0.0013\n","Epoch [4/10], Step [1300/4200], Loss: 0.0004\n","Epoch [4/10], Step [1400/4200], Loss: 0.0008\n","Epoch [4/10], Step [1500/4200], Loss: 0.0007\n","Epoch [4/10], Step [1600/4200], Loss: 0.0004\n","Epoch [4/10], Step [1700/4200], Loss: 0.0009\n","Epoch [4/10], Step [1800/4200], Loss: 0.0007\n","Epoch [4/10], Step [1900/4200], Loss: 0.0009\n","Epoch [4/10], Step [2000/4200], Loss: 0.0022\n","Epoch [4/10], Step [2100/4200], Loss: 0.0014\n","Epoch [4/10], Step [2200/4200], Loss: 0.0011\n","Epoch [4/10], Step [2300/4200], Loss: 0.0019\n","Epoch [4/10], Step [2400/4200], Loss: 0.0013\n","Epoch [4/10], Step [2500/4200], Loss: 0.0023\n","Epoch [4/10], Step [2600/4200], Loss: 0.0007\n","Epoch [4/10], Step [2700/4200], Loss: 0.0013\n","Epoch [4/10], Step [2800/4200], Loss: 0.0005\n","Epoch [4/10], Step [2900/4200], Loss: 0.0003\n","Epoch [4/10], Step [3000/4200], Loss: 0.0006\n","Epoch [4/10], Step [3100/4200], Loss: 0.0005\n","Epoch [4/10], Step [3200/4200], Loss: 0.0004\n","Epoch [4/10], Step [3300/4200], Loss: 0.0005\n","Epoch [4/10], Step [3400/4200], Loss: 0.0014\n","Epoch [4/10], Step [3500/4200], Loss: 0.0005\n","Epoch [4/10], Step [3600/4200], Loss: 0.0008\n","Epoch [4/10], Step [3700/4200], Loss: 0.0005\n","Epoch [4/10], Step [3800/4200], Loss: 0.0036\n","Epoch [4/10], Step [3900/4200], Loss: 0.0035\n","Epoch [4/10], Step [4000/4200], Loss: 0.0030\n","Epoch [4/10], Step [4100/4200], Loss: 0.0034\n","Epoch [4/10], Step [4200/4200], Loss: 0.0014\n","Epoch [5/10], Step [100/4200], Loss: 0.0013\n","Epoch [5/10], Step [200/4200], Loss: 0.0005\n","Epoch [5/10], Step [300/4200], Loss: 0.0005\n","Epoch [5/10], Step [400/4200], Loss: 0.0005\n","Epoch [5/10], Step [500/4200], Loss: 0.0009\n","Epoch [5/10], Step [600/4200], Loss: 0.0037\n","Epoch [5/10], Step [700/4200], Loss: 0.0074\n","Epoch [5/10], Step [800/4200], Loss: 0.0013\n","Epoch [5/10], Step [900/4200], Loss: 0.0012\n","Epoch [5/10], Step [1000/4200], Loss: 0.0005\n","Epoch [5/10], Step [1100/4200], Loss: 0.0005\n","Epoch [5/10], Step [1200/4200], Loss: 0.0004\n","Epoch [5/10], Step [1300/4200], Loss: 0.0004\n","Epoch [5/10], Step [1400/4200], Loss: 0.0005\n","Epoch [5/10], Step [1500/4200], Loss: 0.0007\n","Epoch [5/10], Step [1600/4200], Loss: 0.0011\n","Epoch [5/10], Step [1700/4200], Loss: 0.0004\n","Epoch [5/10], Step [1800/4200], Loss: 0.0005\n","Epoch [5/10], Step [1900/4200], Loss: 0.0002\n","Epoch [5/10], Step [2000/4200], Loss: 0.0009\n","Epoch [5/10], Step [2100/4200], Loss: 0.0002\n","Epoch [5/10], Step [2200/4200], Loss: 0.0006\n","Epoch [5/10], Step [2300/4200], Loss: 0.0004\n","Epoch [5/10], Step [2400/4200], Loss: 0.0028\n","Epoch [5/10], Step [2500/4200], Loss: 0.0009\n","Epoch [5/10], Step [2600/4200], Loss: 0.0011\n","Epoch [5/10], Step [2700/4200], Loss: 0.0007\n","Epoch [5/10], Step [2800/4200], Loss: 0.0011\n","Epoch [5/10], Step [2900/4200], Loss: 0.0003\n","Epoch [5/10], Step [3000/4200], Loss: 0.0002\n","Epoch [5/10], Step [3100/4200], Loss: 0.0016\n","Epoch [5/10], Step [3200/4200], Loss: 0.0013\n","Epoch [5/10], Step [3300/4200], Loss: 0.0006\n","Epoch [5/10], Step [3400/4200], Loss: 0.0012\n","Epoch [5/10], Step [3500/4200], Loss: 0.0003\n","Epoch [5/10], Step [3600/4200], Loss: 0.0002\n","Epoch [5/10], Step [3700/4200], Loss: 0.0009\n","Epoch [5/10], Step [3800/4200], Loss: 0.0001\n","Epoch [5/10], Step [3900/4200], Loss: 0.0004\n","Epoch [5/10], Step [4000/4200], Loss: 0.0004\n","Epoch [5/10], Step [4100/4200], Loss: 0.0021\n","Epoch [5/10], Step [4200/4200], Loss: 0.0002\n","Epoch [6/10], Step [100/4200], Loss: 0.0012\n","Epoch [6/10], Step [200/4200], Loss: 0.0005\n","Epoch [6/10], Step [300/4200], Loss: 0.0010\n","Epoch [6/10], Step [400/4200], Loss: 0.0012\n","Epoch [6/10], Step [500/4200], Loss: 0.0012\n","Epoch [6/10], Step [600/4200], Loss: 0.0012\n","Epoch [6/10], Step [700/4200], Loss: 0.0004\n","Epoch [6/10], Step [800/4200], Loss: 0.0003\n","Epoch [6/10], Step [900/4200], Loss: 0.0004\n","Epoch [6/10], Step [1000/4200], Loss: 0.0003\n","Epoch [6/10], Step [1100/4200], Loss: 0.0004\n","Epoch [6/10], Step [1200/4200], Loss: 0.0003\n","Epoch [6/10], Step [1300/4200], Loss: 0.0129\n","Epoch [6/10], Step [1400/4200], Loss: 0.0020\n","Epoch [6/10], Step [1500/4200], Loss: 0.0005\n","Epoch [6/10], Step [1600/4200], Loss: 0.0005\n","Epoch [6/10], Step [1700/4200], Loss: 0.0012\n","Epoch [6/10], Step [1800/4200], Loss: 0.0005\n","Epoch [6/10], Step [1900/4200], Loss: 0.0006\n","Epoch [6/10], Step [2000/4200], Loss: 0.0003\n","Epoch [6/10], Step [2100/4200], Loss: 0.0009\n","Epoch [6/10], Step [2200/4200], Loss: 0.0015\n","Epoch [6/10], Step [2300/4200], Loss: 0.0017\n","Epoch [6/10], Step [2400/4200], Loss: 0.0007\n","Epoch [6/10], Step [2500/4200], Loss: 0.0047\n","Epoch [6/10], Step [2600/4200], Loss: 0.0015\n","Epoch [6/10], Step [2700/4200], Loss: 0.0007\n","Epoch [6/10], Step [2800/4200], Loss: 0.0004\n","Epoch [6/10], Step [2900/4200], Loss: 0.0005\n","Epoch [6/10], Step [3000/4200], Loss: 0.0002\n","Epoch [6/10], Step [3100/4200], Loss: 0.0006\n","Epoch [6/10], Step [3200/4200], Loss: 0.0004\n","Epoch [6/10], Step [3300/4200], Loss: 0.0016\n","Epoch [6/10], Step [3400/4200], Loss: 0.0003\n","Epoch [6/10], Step [3500/4200], Loss: 0.0005\n","Epoch [6/10], Step [3600/4200], Loss: 0.0006\n","Epoch [6/10], Step [3700/4200], Loss: 0.0003\n","Epoch [6/10], Step [3800/4200], Loss: 0.0014\n","Epoch [6/10], Step [3900/4200], Loss: 0.0004\n","Epoch [6/10], Step [4000/4200], Loss: 0.0003\n","Epoch [6/10], Step [4100/4200], Loss: 0.0005\n","Epoch [6/10], Step [4200/4200], Loss: 0.0003\n","Epoch [7/10], Step [100/4200], Loss: 0.0008\n","Epoch [7/10], Step [200/4200], Loss: 0.0001\n","Epoch [7/10], Step [300/4200], Loss: 0.0003\n","Epoch [7/10], Step [400/4200], Loss: 0.0002\n","Epoch [7/10], Step [500/4200], Loss: 0.0008\n","Epoch [7/10], Step [600/4200], Loss: 0.0002\n","Epoch [7/10], Step [700/4200], Loss: 0.0009\n","Epoch [7/10], Step [800/4200], Loss: 0.0003\n","Epoch [7/10], Step [900/4200], Loss: 0.0003\n","Epoch [7/10], Step [1000/4200], Loss: 0.0001\n","Epoch [7/10], Step [1100/4200], Loss: 0.0022\n","Epoch [7/10], Step [1200/4200], Loss: 0.0006\n","Epoch [7/10], Step [1300/4200], Loss: 0.0010\n","Epoch [7/10], Step [1400/4200], Loss: 0.0011\n","Epoch [7/10], Step [1500/4200], Loss: 0.0033\n","Epoch [7/10], Step [1600/4200], Loss: 0.0005\n","Epoch [7/10], Step [1700/4200], Loss: 0.0015\n","Epoch [7/10], Step [1800/4200], Loss: 0.0005\n","Epoch [7/10], Step [1900/4200], Loss: 0.0081\n","Epoch [7/10], Step [2000/4200], Loss: 0.0015\n","Epoch [7/10], Step [2100/4200], Loss: 0.0007\n","Epoch [7/10], Step [2200/4200], Loss: 0.0020\n","Epoch [7/10], Step [2300/4200], Loss: 0.0008\n","Epoch [7/10], Step [2400/4200], Loss: 0.0005\n","Epoch [7/10], Step [2500/4200], Loss: 0.0046\n","Epoch [7/10], Step [2600/4200], Loss: 0.0008\n","Epoch [7/10], Step [2700/4200], Loss: 0.0008\n","Epoch [7/10], Step [2800/4200], Loss: 0.0027\n","Epoch [7/10], Step [2900/4200], Loss: 0.0025\n","Epoch [7/10], Step [3000/4200], Loss: 0.0007\n","Epoch [7/10], Step [3100/4200], Loss: 0.0006\n","Epoch [7/10], Step [3200/4200], Loss: 0.0018\n","Epoch [7/10], Step [3300/4200], Loss: 0.0004\n","Epoch [7/10], Step [3400/4200], Loss: 0.0018\n","Epoch [7/10], Step [3500/4200], Loss: 0.0006\n","Epoch [7/10], Step [3600/4200], Loss: 0.0010\n","Epoch [7/10], Step [3700/4200], Loss: 0.0003\n","Epoch [7/10], Step [3800/4200], Loss: 0.0004\n","Epoch [7/10], Step [3900/4200], Loss: 0.0003\n","Epoch [7/10], Step [4000/4200], Loss: 0.0001\n","Epoch [7/10], Step [4100/4200], Loss: 0.0002\n","Epoch [7/10], Step [4200/4200], Loss: 0.0006\n","Epoch [8/10], Step [100/4200], Loss: 0.0011\n","Epoch [8/10], Step [200/4200], Loss: 0.0018\n","Epoch [8/10], Step [300/4200], Loss: 0.0024\n","Epoch [8/10], Step [400/4200], Loss: 0.0004\n","Epoch [8/10], Step [500/4200], Loss: 0.0003\n","Epoch [8/10], Step [600/4200], Loss: 0.0006\n","Epoch [8/10], Step [700/4200], Loss: 0.0002\n","Epoch [8/10], Step [800/4200], Loss: 0.0010\n","Epoch [8/10], Step [900/4200], Loss: 0.0003\n","Epoch [8/10], Step [1000/4200], Loss: 0.0003\n","Epoch [8/10], Step [1100/4200], Loss: 0.0002\n","Epoch [8/10], Step [1200/4200], Loss: 0.0002\n","Epoch [8/10], Step [1300/4200], Loss: 0.0009\n","Epoch [8/10], Step [1400/4200], Loss: 0.0004\n","Epoch [8/10], Step [1500/4200], Loss: 0.0003\n","Epoch [8/10], Step [1600/4200], Loss: 0.0002\n","Epoch [8/10], Step [1700/4200], Loss: 0.0003\n","Epoch [8/10], Step [1800/4200], Loss: 0.0001\n","Epoch [8/10], Step [1900/4200], Loss: 0.0003\n","Epoch [8/10], Step [2000/4200], Loss: 0.0016\n","Epoch [8/10], Step [2100/4200], Loss: 0.0004\n","Epoch [8/10], Step [2200/4200], Loss: 0.0003\n","Epoch [8/10], Step [2300/4200], Loss: 0.0005\n","Epoch [8/10], Step [2400/4200], Loss: 0.0002\n","Epoch [8/10], Step [2500/4200], Loss: 0.0001\n","Epoch [8/10], Step [2600/4200], Loss: 0.0019\n","Epoch [8/10], Step [2700/4200], Loss: 0.0077\n","Epoch [8/10], Step [2800/4200], Loss: 0.0007\n","Epoch [8/10], Step [2900/4200], Loss: 0.0010\n","Epoch [8/10], Step [3000/4200], Loss: 0.0022\n","Epoch [8/10], Step [3100/4200], Loss: 0.0005\n","Epoch [8/10], Step [3200/4200], Loss: 0.0006\n","Epoch [8/10], Step [3300/4200], Loss: 0.0004\n","Epoch [8/10], Step [3400/4200], Loss: 0.0004\n","Epoch [8/10], Step [3500/4200], Loss: 0.0015\n","Epoch [8/10], Step [3600/4200], Loss: 0.0004\n","Epoch [8/10], Step [3700/4200], Loss: 0.0001\n","Epoch [8/10], Step [3800/4200], Loss: 0.0002\n","Epoch [8/10], Step [3900/4200], Loss: 0.0002\n","Epoch [8/10], Step [4000/4200], Loss: 0.0002\n","Epoch [8/10], Step [4100/4200], Loss: 0.0003\n","Epoch [8/10], Step [4200/4200], Loss: 0.0005\n","Epoch [9/10], Step [100/4200], Loss: 0.0003\n","Epoch [9/10], Step [200/4200], Loss: 0.0008\n","Epoch [9/10], Step [300/4200], Loss: 0.0004\n","Epoch [9/10], Step [400/4200], Loss: 0.0002\n","Epoch [9/10], Step [500/4200], Loss: 0.0005\n","Epoch [9/10], Step [600/4200], Loss: 0.0005\n","Epoch [9/10], Step [700/4200], Loss: 0.0003\n","Epoch [9/10], Step [800/4200], Loss: 0.0008\n","Epoch [9/10], Step [900/4200], Loss: 0.0005\n","Epoch [9/10], Step [1000/4200], Loss: 0.0001\n","Epoch [9/10], Step [1100/4200], Loss: 0.0013\n","Epoch [9/10], Step [1200/4200], Loss: 0.0002\n","Epoch [9/10], Step [1300/4200], Loss: 0.0033\n","Epoch [9/10], Step [1400/4200], Loss: 0.0011\n","Epoch [9/10], Step [1500/4200], Loss: 0.0013\n","Epoch [9/10], Step [1600/4200], Loss: 0.0003\n","Epoch [9/10], Step [1700/4200], Loss: 0.0028\n","Epoch [9/10], Step [1800/4200], Loss: 0.0008\n","Epoch [9/10], Step [1900/4200], Loss: 0.0014\n","Epoch [9/10], Step [2000/4200], Loss: 0.0002\n","Epoch [9/10], Step [2100/4200], Loss: 0.0011\n","Epoch [9/10], Step [2200/4200], Loss: 0.0009\n","Epoch [9/10], Step [2300/4200], Loss: 0.0001\n","Epoch [9/10], Step [2400/4200], Loss: 0.0009\n","Epoch [9/10], Step [2500/4200], Loss: 0.0003\n","Epoch [9/10], Step [2600/4200], Loss: 0.0001\n","Epoch [9/10], Step [2700/4200], Loss: 0.0001\n","Epoch [9/10], Step [2800/4200], Loss: 0.0001\n","Epoch [9/10], Step [2900/4200], Loss: 0.0001\n","Epoch [9/10], Step [3000/4200], Loss: 0.0008\n","Epoch [9/10], Step [3100/4200], Loss: 0.0019\n","Epoch [9/10], Step [3200/4200], Loss: 0.0004\n","Epoch [9/10], Step [3300/4200], Loss: 0.0006\n","Epoch [9/10], Step [3400/4200], Loss: 0.0004\n","Epoch [9/10], Step [3500/4200], Loss: 0.0002\n","Epoch [9/10], Step [3600/4200], Loss: 0.0012\n","Epoch [9/10], Step [3700/4200], Loss: 0.0027\n","Epoch [9/10], Step [3800/4200], Loss: 0.0008\n","Epoch [9/10], Step [3900/4200], Loss: 0.0021\n","Epoch [9/10], Step [4000/4200], Loss: 0.0004\n","Epoch [9/10], Step [4100/4200], Loss: 0.0004\n","Epoch [9/10], Step [4200/4200], Loss: 0.0003\n","Epoch [10/10], Step [100/4200], Loss: 0.0001\n","Epoch [10/10], Step [200/4200], Loss: 0.0017\n","Epoch [10/10], Step [300/4200], Loss: 0.0001\n","Epoch [10/10], Step [400/4200], Loss: 0.0004\n","Epoch [10/10], Step [500/4200], Loss: 0.0002\n","Epoch [10/10], Step [600/4200], Loss: 0.0016\n","Epoch [10/10], Step [700/4200], Loss: 0.0044\n","Epoch [10/10], Step [800/4200], Loss: 0.0012\n","Epoch [10/10], Step [900/4200], Loss: 0.0005\n","Epoch [10/10], Step [1000/4200], Loss: 0.0002\n","Epoch [10/10], Step [1100/4200], Loss: 0.0003\n","Epoch [10/10], Step [1200/4200], Loss: 0.0004\n","Epoch [10/10], Step [1300/4200], Loss: 0.0001\n","Epoch [10/10], Step [1400/4200], Loss: 0.0013\n","Epoch [10/10], Step [1500/4200], Loss: 0.0005\n","Epoch [10/10], Step [1600/4200], Loss: 0.0003\n","Epoch [10/10], Step [1700/4200], Loss: 0.0011\n","Epoch [10/10], Step [1800/4200], Loss: 0.0006\n","Epoch [10/10], Step [1900/4200], Loss: 0.0001\n","Epoch [10/10], Step [2000/4200], Loss: 0.0015\n","Epoch [10/10], Step [2100/4200], Loss: 0.0122\n","Epoch [10/10], Step [2200/4200], Loss: 0.0002\n","Epoch [10/10], Step [2300/4200], Loss: 0.0002\n","Epoch [10/10], Step [2400/4200], Loss: 0.0009\n","Epoch [10/10], Step [2500/4200], Loss: 0.0008\n","Epoch [10/10], Step [2600/4200], Loss: 0.0001\n","Epoch [10/10], Step [2700/4200], Loss: 0.0008\n","Epoch [10/10], Step [2800/4200], Loss: 0.0015\n","Epoch [10/10], Step [2900/4200], Loss: 0.0005\n","Epoch [10/10], Step [3000/4200], Loss: 0.0001\n","Epoch [10/10], Step [3100/4200], Loss: 0.0002\n","Epoch [10/10], Step [3200/4200], Loss: 0.0003\n","Epoch [10/10], Step [3300/4200], Loss: 0.0003\n","Epoch [10/10], Step [3400/4200], Loss: 0.0002\n","Epoch [10/10], Step [3500/4200], Loss: 0.0006\n","Epoch [10/10], Step [3600/4200], Loss: 0.0001\n","Epoch [10/10], Step [3700/4200], Loss: 0.0001\n","Epoch [10/10], Step [3800/4200], Loss: 0.0002\n","Epoch [10/10], Step [3900/4200], Loss: 0.0017\n","Epoch [10/10], Step [4000/4200], Loss: 0.0005\n","Epoch [10/10], Step [4100/4200], Loss: 0.0034\n","Epoch [10/10], Step [4200/4200], Loss: 0.0002\n"]}],"source":["# Freeze CNN parameters\n","for param in conv_model.parameters():\n","    param.requires_grad = False\n","\n","if torch.cuda.is_available():\n","    conv_model = conv_model.cuda()\n","    puppeteer_net = puppeteer_net.cuda()\n","\n","# Define optimizer for the Puppeteer MLP\n","optimizer = optim.Adam(puppeteer_net.parameters(), lr=0.001)\n","\n","# Training loop parameters\n","num_epochs = 10\n","\n","for epoch in range(num_epochs):\n","    puppeteer_net.train()\n","    running_loss = 0.0\n","\n","    for i, (images, labels) in enumerate(train_loader):\n","        images = images.unsqueeze(1)\n","        if torch.cuda.is_available():\n","            images = images.cuda()\n","            labels = labels.cuda()\n","        \n","        batch_size = images.size(0)\n","        \n","        # Select digits to inhibit (using true labels for simplicity)\n","        inhibited_digits = labels  # Alternatively, use random digits to inhibit\n","        \n","        # Convert inhibited digits to one-hot vectors\n","        inhibited_digits_one_hot = torch.zeros(batch_size, 10).to(images.device)\n","        inhibited_digits_one_hot.scatter_(1, inhibited_digits.view(-1, 1), 1)\n","        \n","        # Forward pass through conv_block of the CNN\n","        x = conv_model.conv_block(images)\n","        \n","        # Flatten the activations\n","        x = x.view(batch_size, -1)\n","        \n","        # Get adjustments from Puppeteer MLP\n","        adjustments = puppeteer_net(inhibited_digits_one_hot)\n","        \n","        # Adjust the activations\n","        x = x + adjustments  # You can experiment with other operations like multiplication\n","        \n","        # Pass adjusted activations through linear_block of the CNN\n","        output = conv_model.linear_block(x)\n","        \n","        # Compute softmax probabilities\n","        output_probs = F.softmax(output, dim=1)\n","        \n","        # Probability of predicting the inhibited digit\n","        p_inhibited = output_probs[range(batch_size), inhibited_digits]\n","        \n","        # Loss function to penalize high probability of inhibited digit\n","        epsilon = 1e-6  # Small value to prevent log(0)\n","        loss = -torch.log(1 - p_inhibited + epsilon)\n","        loss = torch.mean(loss)\n","        \n","        # Backpropagation\n","        optimizer.zero_grad()\n","        loss.backward()\n","        optimizer.step()\n","        \n","        running_loss += loss.item()\n","        \n","        # Print training progress\n","        if (i+1) % 100 == 0:\n","            print(f'Epoch [{epoch+1}/{num_epochs}], Step [{i+1}/{len(train_loader)}], Loss: {running_loss/100:.4f}')\n","            running_loss = 0.0\n"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.15"}},"nbformat":4,"nbformat_minor":1}
